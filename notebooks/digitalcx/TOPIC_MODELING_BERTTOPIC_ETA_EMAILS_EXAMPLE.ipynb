{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b713295-ed58-4c5a-bd22-7337e299c8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "## reading in documents\n",
    "dfcomments = pd.read_excel(\"<your-emails>.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acddf4b-e601-443c-83ca-9265348a1141",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1005e06-8632-43d8-ad6c-fec9dc7808e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# NLP Imports\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    " \n",
    "ENGLISH_STOPWORDS = set(nltk.corpus.stopwords.words('english'))\n",
    "NON_ENGLISH_STOPWORDS = set(nltk.corpus.stopwords.words()) - ENGLISH_STOPWORDS\n",
    "ENGLISH_STOPWORDS.update([' ' ,'s',\"/\", \"'s\",\"also\", \"e.g.\",])\n",
    "STOPWORDS_DICT = {lang: set(nltk.corpus.stopwords.words(lang)) for lang in nltk.corpus.stopwords.fileids()}\n",
    " \n",
    "def get_language(text):\n",
    "    words = set(nltk.wordpunct_tokenize(text.lower()))\n",
    "    return max(((lang, len(words & stopwords)) for lang, stopwords in STOPWORDS_DICT.items()), key = lambda x: x[1])[0]\n",
    "\n",
    "def is_english(text):\n",
    "    text = text.lower()\n",
    "    words = set(nltk.wordpunct_tokenize(text))\n",
    "    return len(words & ENGLISH_STOPWORDS) > len(words & NON_ENGLISH_STOPWORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2530537d-4720-475a-837f-c3b766cd808f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8854, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qm/k3t85_xs2hv5bxqnxxvbxcfm0000gn/T/ipykernel_4803/121717112.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfsubset_[\"text\"] = dfcomments[\"SUBJECT\"].map(str) + \": \" + dfcomments[\"SENDER_COMMENTS\"].map(str)\n"
     ]
    }
   ],
   "source": [
    "dfsubset_ = dfcomments[(dfcomments[\"SUBJECT\"].map(lambda x: is_english(str(x)))) | (dfcomments[\"SENDER_COMMENTS\"].map(lambda x: is_english(str(x))))]\n",
    "print(dfsubset_.shape)\n",
    "dfsubset_[\"text\"] = dfcomments[\"SUBJECT\"].map(str) + \": \" + dfcomments[\"SENDER_COMMENTS\"].map(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84e4e7af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8854, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qm/k3t85_xs2hv5bxqnxxvbxcfm0000gn/T/ipykernel_4803/2141980242.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfsubset_[\"text\"] = dfcomments[\"SUBJECT\"].map(str) + \": \" + dfcomments[\"SENDER_COMMENTS\"].map(str)\n"
     ]
    }
   ],
   "source": [
    "dfsubset_ = dfcomments[(dfcomments[\"SUBJECT\"].map(lambda x: is_english(str(x)))) | (dfcomments[\"SENDER_COMMENTS\"].map(lambda x: is_english(str(x))))]\n",
    "#dfchatbotactivitysitesearch[\"KEYWORD\"].map(lambda x: is_english(str(x)))]\n",
    "print(dfsubset_.shape)\n",
    "dfsubset_[\"text\"] = dfcomments[\"SUBJECT\"].map(str) + \": \" + dfcomments[\"SENDER_COMMENTS\"].map(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b57fedf4-5dea-44df-9c2f-7befdb911fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/isabelmetzger/miniconda3/envs/cxenv/lib/python3.9/site-packages/ekphrasis/classes/tokenizer.py:225: FutureWarning: Possible nested set at position 2190\n",
      "  self.tok = re.compile(r\"({})\".format(\"|\".join(pipeline)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter - 1grams ...\n",
      "Reading twitter - 2grams ...\n",
      "Reading twitter - 1grams ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/isabelmetzger/miniconda3/envs/cxenv/lib/python3.9/site-packages/ekphrasis/classes/exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
      "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n"
     ]
    }
   ],
   "source": [
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "from ekphrasis.dicts.emoticons import emoticons\n",
    "\n",
    "text_processor = TextPreProcessor(\n",
    "    # terms that will be normalized\n",
    "    normalize=['email', 'percent', 'money', 'phone', 'user',\n",
    "        'time', 'url', 'date', 'number'],\n",
    "    # # terms that will be annotated\n",
    "    # annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
    "    #     'emphasis', 'censored'},\n",
    "    fix_html=True,  # fix HTML tokens\n",
    "    \n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for word segmentation \n",
    "    segmenter=\"twitter\", \n",
    "    \n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for spell correction\n",
    "    corrector=\"twitter\", \n",
    "    \n",
    "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
    "    unpack_contractions=True,  # Unpack contractions (can't -> can not)\n",
    "    spell_correct_elong=False,  # spell correction for elongated words\n",
    "    \n",
    "    # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
    "    # the tokenizer, should take as input a string and return a list of tokens\n",
    "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
    "    \n",
    "    # list of dictionaries, for replacing tokens extracted from the text,\n",
    "    # with other expressions. You can pass more than one dictionaries.\n",
    "    dicts=[emoticons]\n",
    " )\n",
    "# docs = dfsubset_[\"text\"].map(lambda x: str(x).replace(\"_x000D_\", \"\")).map(lambda x: \" \".join(text_processor.pre_process_doc(str(x)))).dropna().tolist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde0d6fb-9f0c-48d0-98c0-5220b26e21f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686d7ed6-1679-4bab-8ceb-73a3cbdc013e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8854 emails "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d456d92b-4a06-4f85-a01e-2814fc802dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdbscan import HDBSCAN\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Pre-calculate embeddings\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embeddings = embedding_model.encode(docs, show_progress_bar=True)\n",
    "from umap import UMAP\n",
    "\n",
    "umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine', random_state=42)\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=35, metric='euclidean', cluster_selection_method='eom', prediction_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc3f720-780a-4968-a381-2b8b7d1711f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\", min_df=2, ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613d626e-4d81-4565-9ea5-dc6e6362a5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance, OpenAI, PartOfSpeech\n",
    "\n",
    "# KeyBERT\n",
    "keybert_model = KeyBERTInspired()\n",
    "\n",
    "# Part-of-Speech\n",
    "pos_model = PartOfSpeech(\"en_core_web_md\")\n",
    "\n",
    "# MMR\n",
    "mmr_model = MaximalMarginalRelevance(diversity=0.3)\n",
    "\n",
    "prompt = \"\"\"\n",
    "I have a topic that contains the following documents:\n",
    "[DOCUMENTS]\n",
    "The topic is described by the following keywords: [KEYWORDS]\n",
    "\n",
    "Based on the information above, extract a short but highly descriptive topic label of at most 6 words. Make sure it is in the following format:\n",
    "topic: <topic label>\n",
    "\"\"\"\n",
    "openai_model = OpenAI(model=\"gpt-3.5-turbo\", exponential_backoff=True, chat=True, prompt=prompt)\n",
    "\n",
    "# All representation models\n",
    "representation_model = {\n",
    "    \"KeyBERT\": keybert_model,\n",
    "    \"MMR\": mmr_model,\n",
    "    \"POS\": pos_model\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58248c02-df1e-4493-b5d5-622d463b9197",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "\n",
    "topic_model = BERTopic(\n",
    "\n",
    "  # Pipeline models\n",
    "  embedding_model=embedding_model,\n",
    "  umap_model=umap_model,\n",
    "  hdbscan_model=hdbscan_model,\n",
    "  vectorizer_model=vectorizer_model,\n",
    "  representation_model=representation_model,\n",
    "\n",
    "  # Hyperparameters\n",
    "  top_n_words=10,\n",
    "  verbose=True\n",
    ")\n",
    "\n",
    "topics, probs = topic_model.fit_transform(docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7823a8-d935-47db-97ad-2ee6091ccba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_topic_info()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283dc6b0-6f83-45b8-adf0-4f8f88625f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "pprint.pprint(topic_model.get_topic(1, full=True))\n",
    "# or use one of the other topic representations, like KeyBERTInspired\n",
    "keybert_topic_labels = {topic: \" | \".join(list(zip(*values))[0][:4]) for topic, values in topic_model.topic_aspects_[\"KeyBERT\"].items()}\n",
    "topic_model.set_topic_labels(keybert_topic_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7193d4-814e-4469-8ffa-d4d3a86a38ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(topic_model.get_topic_info())\n",
    "topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b7c098-0e69-40ef-a814-b7984875bb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# or ChatGPT's labels\n",
    "chatgpt_topic_labels = {topic: \" | \".join(list(zip(*values))[0]) for topic, values in topic_model.topic_aspects_[\"Open\"].items()}\n",
    "chatgpt_topic_labels[-1] = \"Outlier Topic\"\n",
    "topic_model.set_topic_labels(chatgpt_topic_labels)\n",
    "\n",
    "# # Reduce outliers\n",
    "# new_topics = topic_model.reduce_outliers(abstracts, topics)\n",
    "\n",
    "# # Reduce outliers with pre-calculate embeddings instead\n",
    "# new_topics = topic_model.reduce_outliers(abstracts, topics, strategy=\"embeddings\", embeddings=embeddings)\n",
    "topic_model.visualize_topics(custom_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a463dec5-65d9-47fd-bc74-3bbf166764a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPICDF_1_ = topic_model.get_document_info(docs)\n",
    "TOPICDF_1_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48443f80-fc64-4c74-a8b1-f308508164c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TOPICDF_1_[\"CustomName\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc6cf45-fe71-4025-970d-d55ffc089c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TOPICDF_1_[\"Name\"].value_counts()[:10])\n",
    "\n",
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce731e06-15d9-439a-84af-33c46d0d41ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TOPICDF_1_.to_excel(\"/Users/isabelmetzger/Downloads/EXPORT_WITH_TOPIC_MODEL-NEW.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11587407-1c4c-40f3-a9be-987319bd47f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90025b98-7d89-4d9b-b3dd-f1f94841455d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# `topic_distr` contains the distribution of topics in each document\n",
    "topic_distr, _ = topic_model.approximate_distribution(docs, window=4, stride=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131c02ba-4ba6-4e95-8d50-9ec10337258a",
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_id = 1\n",
    "print(docs[abstract_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65247fb-6a43-4028-a858-c9165cba6f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the topic distributions on a token-level\n",
    "topic_distr, topic_token_distr = topic_model.approximate_distribution(docs, calculate_tokens=True)\n",
    "\n",
    "# Visualize the token-level distributions\n",
    "df = topic_model.visualize_approximate_distribution(docs[1], topic_token_distr[1])\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e90edb5-55a2-4c60-9fb5-afe60c4dc03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topic-docuament distribution for a single document\n",
    "topic_model.visualize_distribution(topic_distr[1], custom_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18bda75-ad3b-4c93-9826-4fe4d1f70d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the topic distributions on a token-level\n",
    "abstract_id = 13\n",
    "print(docs[abstract_id])\n",
    "topic_distr, topic_token_distr = topic_model.approximate_distribution(docs[abstract_id], calculate_tokens=True)\n",
    "\n",
    "# Visualize the token-level distributions\n",
    "df = topic_model.visualize_approximate_distribution(docs[abstract_id], topic_token_distr[0])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c48686-1793-4cc2-a090-94449b9a6aa3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a32b18-1fed-470f-b734-7e063312990c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce outliers\n",
    "#new_topics = topic_model.reduce_outliers(docs, topics)\n",
    "\n",
    "# # Reduce outliers with pre-calculate embeddings instead\n",
    "# new_topics = topic_model.reduce_outliers(docs, topics, strategy=\"embeddings\", embeddings=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c5f391-eeb5-4e93-8d7e-fd14b04fed7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#topic_model.update_topics(docs, topics=new_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eed284b-229b-413f-9961-1d733938ff03",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_document_info(docs)[\"CustomName\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6feffe-0280-4692-a247-c4180987cf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchical_topics = topic_model.hierarchical_topics(docs)\n",
    "topic_model.visualize_hierarchy(hierarchical_topics=hierarchical_topics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bec260e-c2f6-4c59-87b3-a291d2869ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = topic_model.get_topic_tree(hierarchical_topics)\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ff00cc-7650-4cdf-964e-fdabaae31a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from bertopic import BERTopic\n",
    "\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "import string\n",
    "\n",
    "def cln(i, extent=1):\n",
    "    \"\"\"\n",
    "    String white space 'cleaner'.\n",
    "    :param i:\n",
    "    :param extent: 1 --> all white space reduced to length 1; 2 --> removal of all white space.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(i, str) and i != \"\":\n",
    "        if extent == 1:\n",
    "            return re.sub(r\"\\s\\s+\", \" \", i)\n",
    "        elif extent == 2:\n",
    "            return re.sub(r\"\\s+\", \"\", i)\n",
    "    else:\n",
    "        return i\n",
    "\n",
    "transtable = str.maketrans(dict.fromkeys(string.punctuation))\n",
    "\n",
    "\n",
    "def strip_punctuation(input_string):\n",
    "    \"\"\"cleans string by stripping punctuation \"\"\"\n",
    "    return input_string.translate(transtable)\n",
    "\n",
    "#ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)\n",
    "model = BERTopic(#ctfidf_model=ctfidf_model,\n",
    "                 verbose=True )\n",
    "# Prepare data\n",
    "\n",
    "timestamps = pd.to_datetime(dfsubset_[\"COMMENT_DATE\"]).tolist()\n",
    "# Create topics over time\n",
    "model = BERTopic(verbose=True)\n",
    "topics, probs = model.fit_transform(docs)\n",
    "topics_over_time = model.topics_over_time(docs, timestamps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54d4c67-7a2c-40f0-ba5d-50c32b494e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hierarchical_topics2 = model.hierarchical_topics(docs)\n",
    "tree2 = model.get_topic_tree(hierarchical_topics2)\n",
    "print(tree2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401f2df0-f58a-4f10-8f47-8272939704aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.visualize_hierarchy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6577d843-1403-44c7-9be5-5e9ce916cc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2761199-f182-4935-8244-f0da6a5bb476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce outliers\n",
    "new_topics = model.reduce_outliers(docs, topics)\n",
    "model.update_topics(docs, topics=new_topics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1161a83c-e047-4ab0-9617-af1f12c2b7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.visualize_topics()#0, 14, 117, 74, 81, 85, 105"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8070b1-f1c3-41a6-94c0-0d3b06995f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_over_time = model.topics_over_time(docs, timestamps, nr_bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7686a7-874f-4d97-8a39-94484f364e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_topic_freq().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4107d5-5585-4350-8ba5-ec7321d298b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502778ac-a804-4aaa-a236-72308b87742a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.visualize_topics_over_time(topics_over_time) #=topics_over_time, top_n_topics=20, topics=[0, 19, 9, 20, 33,37,85, 50 ,14, 117, 81, 85])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9bf22c-62e1-4640-96ed-d24450c7e411",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.visualize_topics_over_time(topics_over_time=topics_over_time, top_n_topics=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c66551-4c5e-4502-919a-250732ebd361",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69be665-a690-480d-925d-eb18417338be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76386748-cd70-4872-8d4b-92d75d6331cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs2 = dfsubset_[dfsubset_[\"CATEGORY\"]!=\"CareerOneStop\"][\"text\"].map(lambda x: cln(str(x).replace('x000d', ' '))).tolist()\n",
    "print(len(docs2))\n",
    "timestamps2 = dfsubset_[dfsubset_[\"CATEGORY\"]!=\"CareerOneStop\"][\"COMMENT_DATE\"].tolist()\n",
    "topics2, probs2 = model.fit_transform(docs2)\n",
    "topics_over_time2 = model.topics_over_time(docs2, timestamps2, nr_bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be327b6a-6a4b-4cf1-9890-536134e6ed56",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.visualize_topics_over_time(topics_over_time=topics_over_time2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3c14ba-eadb-4861-b553-a4be0c738203",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_document_info(docs2).sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d51b4bc-c9e8-44d4-9f70-50d715a1f378",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_topic_info(0) # video messages stopped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a4451a-cb3e-4d5c-a866-18487cf42af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPICDF_1_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "85031da3-da27-4199-a5d8-8eaa1338889f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8854, 13)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOPICDF_1_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9a8792cc-4489-4428-bb8d-a5443589cf01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8854, 6)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfsubset_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3617c0ec-ea81-446c-b752-9d710b3f8cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "DFMERGED1 = dfsubset_.reset_index(drop=True).merge(TOPICDF_1_.reset_index(drop=True), right_index=True, left_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72584155-cc39-4ed1-b346-6bc6c73c524a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "import string\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import ujson\n",
    "\n",
    "import warnings\n",
    " \n",
    "def warning_function():\n",
    "    warnings.warn(\"deprecated\", DeprecationWarning)\n",
    " \n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    warning_function() \n",
    "\n",
    "\n",
    "def to_string(s):\n",
    "    \"\"\"Converts input to a string.\"\"\"\n",
    "    try:\n",
    "        return str(s)\n",
    "    except:\n",
    "        return s.encode('utf-8')\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    \"\"\"Converts unicode to ASCII.\"\"\"\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def create_cardinality_feature(df):\n",
    "    \"\"\"Creates a cardinality feature for a dataframe.\"\"\"\n",
    "    num_rows = len(df)\n",
    "    random_code_list = np.arange(100, 1000, 1)\n",
    "    return np.random.choice(random_code_list, num_rows)\n",
    "\n",
    "def print_current_date():\n",
    "    todaysdate = datetime.datetime.today().strftime('%Y-%m-%d %H:%M')\n",
    "    print(f\"{todaysdate}\")\n",
    "\n",
    "def get_date_timestamp():\n",
    "    todaysdate = datetime.datetime.today().strftime('%Y-%m-%d_%H:%M:%S')\n",
    "    return todaysdate\n",
    "\n",
    "toeval = lambda x: eval(x) if isinstance(x, str) and x[0]=='[' else x\n",
    "tolist = lambda x: x if isinstance(x, list) else (x if pd.isnull(x) else [x])\n",
    "\n",
    "def term_in_val(val, term):\n",
    "    if term in val:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def items_present_test(input_list, clist):\n",
    "    return any(x in input_list for x in clist)\n",
    "\n",
    "\n",
    "def chunks(l, n):\n",
    "    \"\"\"Yield n number of sequential chunks from l.\"\"\"\n",
    "    d, r = divmod(len(l), n)\n",
    "    for i in range(n):\n",
    "        si = (d + 1) * (i if i < r else r) + d * (0 if i < r else i - r)\n",
    "        yield l[si:si + (d + 1 if i < r else d)]\n",
    "\n",
    "\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "def replace_none(val, returnval = '--'):\n",
    "    if val == None:\n",
    "        return returnval\n",
    "    \n",
    "    else:\n",
    "        return val\n",
    "    \n",
    "\n",
    "def matches(input1, input2):\n",
    "    return (input1 == input2)\n",
    "\n",
    "def intersection_two_lists(lst1, lst2):\n",
    "    return set(lst1).intersection(lst2)\n",
    "\n",
    "\n",
    "def cln(i, extent=1):\n",
    "    \"\"\"\n",
    "    String white space 'cleaner'.\n",
    "    :param i:\n",
    "    :param extent: 1 --> all white space reduced to length 1; 2 --> removal of all white space.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(i, str) and i != \"\":\n",
    "        if extent == 1:\n",
    "            return re.sub(r\"\\s\\s+\", \" \", i)\n",
    "        elif extent == 2:\n",
    "            return re.sub(r\"\\s+\", \"\", i)\n",
    "    else:\n",
    "        return i\n",
    "\n",
    "transtable = str.maketrans(dict.fromkeys(string.punctuation))\n",
    "\n",
    "\n",
    "def strip_punctuation(input_string):\n",
    "    \"\"\"cleans string by stripping punctuation \"\"\"\n",
    "    return input_string.translate(transtable)\n",
    "\n",
    "def get_match(text, rex):\n",
    "    if isinstance(rex, (list, tuple, set)):\n",
    "        rex = '(' + '|'.join(rex) + ')'\n",
    "    result = re.findall(rex, text)\n",
    "    return result\n",
    "\n",
    "def partial_match(input_str, looking_for):\n",
    "    \"\"\"\n",
    "    :param input_str:\n",
    "    :param looking_for:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if isinstance(input_str, str) and isinstance(looking_for, str):\n",
    "        return cln(looking_for.lower(), 1) in cln(input_str.lower(), 1)\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def return_no_str(val):\n",
    "\n",
    "    if str(val).lower().strip() in [\"none\",\"nan\", \"n/a\"]:\n",
    "        return None\n",
    "    else: \n",
    "        return val\n",
    "    \n",
    "\n",
    "def listify(l): return l if isinstance(l, (list, tuple, set)) else [l]\n",
    "\n",
    "    \n",
    "def flatten_json(y):\n",
    "    \"\"\"\n",
    "    this flattens_json obj\n",
    "    adds begining key to beginning\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "\n",
    "    def flatten(x, name=''):\n",
    "        if type(x) is dict:\n",
    "            for a in x:\n",
    "                flatten(x[a], name + a + '_')\n",
    "        elif type(x) is list:\n",
    "            i = 0\n",
    "            for a in x:\n",
    "                flatten(a, name + str(i) + '_')\n",
    "                i += 1\n",
    "            out[name[:-1]] = x\n",
    "\n",
    "    flatten(y)\n",
    "    return out\n",
    "\n",
    "\n",
    "def returnNonList(val):\n",
    "    if not isinstance(val, list):\n",
    "        return val\n",
    "    if isinstance(val, list):\n",
    "        if len(val) == 1:\n",
    "            return val[0]\n",
    "        else:\n",
    "            return '|'.join(val)\n",
    "\n",
    "    \n",
    "def gather_categorical_col(df, n=10):\n",
    "    df = df.applymap(returnNonList)\n",
    "    categorical_col = []\n",
    "    for column in df.columns:\n",
    "        if df[column].dtype == object and len(df[column].unique()) <= n:\n",
    "            categorical_col.append(column)\n",
    "\n",
    "            print(f\"{column} : {df[column].unique()}\")\n",
    "            print(\"====================================\")\n",
    "    return categorical_col\n",
    "\n",
    "\n",
    "def make_inv_map(my_map):\n",
    "    inv_map = {v: k for k, v in my_map.items()}\n",
    "    return inv_map\n",
    "\n",
    "\n",
    "def write_jsonl(file_path, lines):\n",
    "    \"\"\"\n",
    "    wrts a .jsonl file and dump contents.\n",
    "    file_path (unicode / Path): The path to the output file.\n",
    "    lines (list): The JSON-serializable contents of each line.\n",
    "    \"\"\"\n",
    "    data = [ujson.dumps(line, escape_forward_slashes=False) for line in lines]\n",
    "    Path(file_path).open('w', encoding='utf-8').write('\\n'.join(data))\n",
    "\n",
    "\n",
    "def make_generator(file_name, sep=\",\"):\n",
    "    \"\"\"this function uses a generator\"\"\"\n",
    "    qlines = (line for line in open(file_name))\n",
    "    list_line = (s.rstrip().split(sep) for s in lines)\n",
    "    cols = next(list_line)\n",
    "    df_dicts = (dict(zip(cols, data)) for data in list_line)\n",
    "    return df_dicts\n",
    "import pandas as pd\n",
    "import torch\n",
    "from IPython import display\n",
    "import jsonlines\n",
    "import numpy as np\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=True, truncation=True)\n",
    "\n",
    "\n",
    "def initialize_emotion_classifier(topk=3):\n",
    "    \"\"\"default returns the top 3 emotions\"\"\"\n",
    "    \n",
    "    classifier = pipeline(\"text-classification\",model='bhadresh-savani/distilbert-base-uncased-emotion', top_k=topk)\n",
    "    return classifier\n",
    "\n",
    "\n",
    "def pred_emotion_classifier(classifier,\n",
    "                             sample_text=\"I love using transformers. The best part is wide range of support and its easy to use\",\n",
    "                             verbose=False):\n",
    "\n",
    "    prediction = classifier(sample_text)\n",
    "    if verbose:\n",
    "        print(prediction)\n",
    "    return prediction\n",
    "\n",
    "\n",
    "\n",
    "def get_text_emotion_preds(classifier, dfcomments, textcol):\n",
    "    ts = get_date_timestamp()\n",
    "\n",
    "    catpredlist = []\n",
    "    dfcomments=dfcomments[dfcomments[textcol].notna()]\n",
    "    N = dfcomments.shape[0]\n",
    "    print(N)\n",
    "\n",
    "    outpath=f\"../{str(ts)}-{str(N)}-{str(textcol).lower().split(' ')[0]}-output-emotion-preds.jsonl\"\n",
    "    catpredlist = []\n",
    "    with jsonlines.open(outpath, \"w\") as fout:\n",
    "        \n",
    "        for d in dfcomments.to_dict(\"records\"):\n",
    "            newd = d.copy()\n",
    "            text = d[textcol]\n",
    "\n",
    "            try: \n",
    "                if len(text) < 10: \n",
    "                    emot = classifier(\"NONE\",)\n",
    "                else:\n",
    "                    emot = classifier(text,)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            newd['emot_topk'] = emot\n",
    "            fout.write(newd)\n",
    "            catpredlist.append(newd)\n",
    "\n",
    "    return catpredlist\n",
    "\n",
    "\n",
    "def turn_catpredlist_to_df(catpredlist):\n",
    "\n",
    "    commentdfwithpreds = pd.DataFrame(catpredlist)\n",
    "    \n",
    "    commentdfwithpreds[\"emotion_top\"] =commentdfwithpreds[\n",
    "        'emot_topk'\n",
    "    ].map(lambda x:  flatten(x)[0]if len(x) == 1  else x[0], na_action='ignore').map(lambda x: x.get('label', x))\n",
    "\n",
    "    commentdfwithpreds[\"emotion_score\"] =  commentdfwithpreds[\n",
    "        'emot_topk'\n",
    "    ].map(lambda x:  flatten(x)[0]if len(x) == 1  else x[0], na_action='ignore').map(lambda x: x.get('score', x))\n",
    "    \n",
    "    \n",
    "    return commentdfwithpreds\n",
    "    \n",
    "\n",
    "def make_dfonlycomments(df, textcol=\"Anything else you\\'d like to share with us?\"):\n",
    "    df[textcol] = df[textcol].map(return_no_str)\n",
    "    print(df.shape[0])\n",
    "\n",
    "    dfonlycomments = df[df[textcol].notna()]\n",
    "    print(dfonlycomments.shape[0])\n",
    "    return dfonlycomments\n",
    "\n",
    "\n",
    "\n",
    "def get_label_val(vl):\n",
    "    nv = {}\n",
    "    for v in vl:\n",
    "        for k, vv in v.items():\n",
    "            nv[k[\"label\"]] = vv[\"score\"]\n",
    "            \n",
    "    return nv\n",
    "\n",
    "\n",
    "def get_emotion_top_stats(dflistpred):\n",
    "    N = dflistpred.shape[0]\n",
    "\n",
    "\n",
    "    dcounts = dflistpred[\"emotion_top\"].value_counts().reset_index().rename(columns={\"emotion_top\": \"counts\", \"index\": \"emotion_top\",\n",
    "                   })\n",
    "    display.display(dcounts)\n",
    "    return dcounts, N\n",
    "\n",
    "\n",
    "\n",
    "def create_group_counts_distribution(dflistpred, groupcol):\n",
    "    \n",
    "    dflistpred[groupcol] = dflistpred[groupcol].fillna(\"N/A\")\n",
    "\n",
    "    countsdf = dflistpred[groupcol].value_counts().to_frame().reset_index().rename(columns={\"count\": \"# Responses\"\n",
    "                                                                                            })\n",
    "    N = countsdf[\"# Responses\"].sum()\n",
    "    countsdf[\"% Responses\"] = np.round(100*countsdf[\"# Responses\"]/ N)\n",
    "\n",
    "    return countsdf\n",
    "\n",
    "\n",
    "def get_emotions_by_groupcol(dflistpred, groupcol):\n",
    "    \n",
    "    dflistpred[groupcol] = dflistpred[groupcol].fillna(\"Did not respond\")\n",
    "    \n",
    "    countsemotionbygroup = pd.crosstab(\n",
    "        dflistpred[\"emotion_top\"], dflistpred[groupcol]\n",
    "        ).reset_index()\n",
    "    return countsemotionbygroup\n",
    "\n",
    "\n",
    "negemotions = [\"sadness\", \"anger\", \"fear\"]\n",
    "posemotions =[\"joy\", \"love\"]\n",
    "\n",
    "def filtercommments(dflistpred,emotionlist=posemotions):\n",
    "    emotion_filtered_comments= (\n",
    "    dflistpred[dflistpred[\"emotion_top\"].isin(emotionlist)].sort_values([\"emotion_score\"], ascending=[False])\n",
    ")\n",
    "    return emotion_filtered_comments\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "def get_vader_sentiment(s):\n",
    "    return analyzer.polarity_scores(s)\n",
    "\n",
    "def get_top_cat(cats):\n",
    "    \"\"\"takes a spacy doc object and returns the category with highest score\"\"\"\n",
    "    max_score = max(cats.values())\n",
    "    max_cats = [k for k, v in cats.items() if v == max_score]\n",
    "    max_cat = max_cats[0]\n",
    "    return (max_cat, max_score)\n",
    "\n",
    "def get_list_more(cats, score=0.4):\n",
    "    l = []\n",
    "    for k, v in cats.items():\n",
    "        if v > score:\n",
    "                \n",
    "            l.append(f\"{str(k)}{str(v)[:3]}\")\n",
    "        else:\n",
    "            continue\n",
    "    return '-'.join(l)\n",
    "\n",
    "def change_2_dict(list_dicts):\n",
    "    nd = {}\n",
    "    for d_ in list_dicts:\n",
    "        nd[d_[\"label\"]] = d_[\"score\"]\n",
    "    return nd\n",
    "    \n",
    "def matches(input1, input2): return (input1 == input2)\n",
    "\n",
    "def get_score_cats(compound_score):\n",
    "    if compound_score >= 0.05:\n",
    "        return \"pos\"\n",
    "    if (-0.05 < compound_score < 0.05):\n",
    "        return \"neu\"\n",
    "    if compound_score <=-0.05:\n",
    "        return \"neg\"\n",
    "    else: return \"N/A\"\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b257e0-f949-4801-a752-e36903ec5aa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bb9677-8d7f-431a-8c19-350d4860ad95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
